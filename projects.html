<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Suleyman Talha Belen | Projects</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta
    name="description"
    content="Projects by Suleyman Talha Belen on reinforcement learning, neural rendering for seismic imaging, radar signal processing, and image super-resolution with SwinIR."
  />
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <header>
    <div class="nav-container">
      <div class="logo"><a href="index.html">S. T. Belen</a></div>
      <nav>
        <a href="education.html">Education</a>
        <a href="experience.html">Experience</a>
        <a href="projects.html">Projects</a>
        <a href="ideas.html">Ideas</a>
        <a href="writings.html">Writings</a>
        <a href="research.html">Research</a>
        <a href="index.html#contact">Contact</a>
      </nav>
    </div>
  </header>

  <main>
    <section class="reveal">
      <h1>Projects</h1>
      <p class="page-subtitle">
        Selected hands-on projects and experiments, mostly around reinforcement learning, neural fields, radar, and image restoration.
      </p>

      <div class="grid">
        <!-- PPO-MAML -->
        <article class="card">
          <img
            src="ppo-maml-assembly.gif"
            alt="PPO-MAML adaptation on Meta-World Assembly-v3"
            class="project-image"
          />
          <h2>PPO-MAML for Meta-World Assembly-v3</h2>
          <p class="meta">
            Reinforcement Learning · Meta RL · Robotic Manipulation
          </p>
          <p>
            A compact PyTorch implementation that combines Model-Agnostic Meta-Learning (MAML) with a PPO inner loop
            on the Meta-World ML1 <code>assembly-v3</code> benchmark. The goal is fast policy adaptation for a
            robotic arm assembling a peg and socket under task variations.
          </p>
          <ul class="list">
            <li>Single ~200-line script with fixed seeds, CSV logging, and GIF rollout for visualizing adaptation.</li>
            <li>“Simple but solid” ingredients: reward normalisation, entropy regularisation, gradient clipping.</li>
            <li>Runs on CPU (MacBook Pro M2) and can be ported to GPU for larger sweeps.</li>
          </ul>
          <p class="meta">
            Code and details:
            <a
              href="https://github.com/stbelen-st/Hands-on-Experiment-Fast-Adaptation-on-Meta-World-Assembly-v3-with-PPO-MAML"
              target="_blank"
              rel="noreferrer"
            >
              GitHub repository
            </a>
          </p>
        </article>

        <!-- Seismic / NeRF -->
        <article class="card" id="seismic-nerf">
          <img
            src="nerf.png"
            alt="Marine seismic acquisition and subsurface illustration"
            class="project-image"
          />
          <h2>Seismic NeRF-inspired Subsurface Imaging</h2>
          <p class="meta">Neural Rendering · Seismic Imaging</p>
          <p>
            I explored how Neural Radiance Fields (NeRF) ideas can transfer from multi-view camera rendering
            to marine seismic acquisition. The focus was on understanding seismic data structure
            (CMP gathers / SEG-Y) and designing a concept pipeline that treats offset and azimuth as “view”
            parameters in a NeRF-like framework.
          </p>
          <p>
            Beyond geometry, I experimented with the idea of tokenising seismic traces using time–frequency
            features plus vector quantisation. The goal is to learn CMP-level patterns with transformer models so
            that interpretation becomes faster, more robust, and potentially more data-efficient than classic workflows.
          </p>
        </article>

        <!-- Radar -->
        <article class="card" id="radar-ml">
          <img
          src="dopler.png"
          alt="Range–Doppler maps from the RAD-DAR dataset"
          class="project-image"
        />
          <h2>Doppler Radar Classification (People / Car / Drone)</h2>
          <p class="meta">Signal Processing · CNN Baseline · RAD-DAR dataset</p>
          <p>
            I built an end-to-end Doppler radar ML pipeline to classify people, cars, and drones from
            range–Doppler maps. Using the RAD-DAR dataset (11 × 61 matrices), the project focuses on clean
            preprocessing and a clear baseline rather than a huge model.
          </p>
          <p>
            The pipeline includes normalisation, balanced train/validation/test splits, and a compact CNN
            architecture with evaluation and visualisation tools to inspect predictions and common confusions.
            The model reaches around 93% test accuracy, and the setup is structured so new architectures can
            easily be plugged in.
          </p>
        </article>

        <!-- SwinIR -->
        <article class="card" id="swinir-sr">
          <div class="project-image-row">
            <img
              src="swinir-lr.png"
              alt="Low-resolution input image for SwinIR"
              class="project-image-half"
            />
            <img
              src="swinir-hr.png"
              alt="High-resolution reconstruction by SwinIR"
              class="project-image-half"
            />
          </div>
          <img
            src="swinir-arch.png"
            alt="SwinIR architecture diagram"
            class="project-image-arch"
          />
          <h2>Image Super-Resolution with SwinIR</h2>
          <p class="meta">Image Restoration · Swin Transformer · Super-Resolution</p>
          <p>
            Implemented and trained SwinIR, a Transformer-based model for Single-Image Super-Resolution (SISR),
            to reconstruct high-resolution images from low-resolution inputs. The model was trained from scratch
            on the <code>LSDIR</code> dataset and evaluated on the <code>Manga109</code> benchmark for ×2 scaling.
          </p>
          <p>
            The implementation achieved about <strong>36.18 dB PSNR (×2)</strong>, outperforming a standard
            CNN-based baseline (<code>RCAN</code>) in both reconstruction quality and parameter efficiency.
            The codebase is set up for ablations on window size, depth, and attention configurations.
          </p>
          <p class="meta">
            Based on:
            J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and Y. Timofte,
            “SwinIR: Image Restoration Using Swin Transformer,” ICCV Workshops (AIM Workshop), 2021.
          </p>
        </article>
      </div>
    </section>
  </main>

  <footer>
    <span>© <span id="year"></span> Suleyman Talha Belen.</span>
    <span
      class="back-to-top"
      onclick="window.scrollTo({ top: 0, behavior: 'smooth' })"
    >
      ↑ Back to top
    </span>
  </footer>

  <script>
    document.getElementById("year").textContent = new Date().getFullYear();

    const reveals = document.querySelectorAll(".reveal");
    const observer = new IntersectionObserver(
      (entries) => {
        entries.forEach((entry) => {
          if (entry.isIntersecting) {
            entry.target.classList.add("active");
            observer.unobserve(entry.target);
          }
        });
      },
      { threshold: 0.12 }
    );
    reveals.forEach((el) => observer.observe(el));
  </script>
</body>
</html>